import re
import numpy as np
from collections import defaultdict
import math

class HMMSegmenter(object):
    def __init__(self,corpus):
        self.corpus=corpus
        self.tag_set=['B','E','M','S']
        self.char_set=get_set([w for w,t in self.corpus])
          
    def train(self):
        char_freq=ConditionalFreqDist(self.tag_set,self.char_set,self.corpus)
        smooth_char_freq=AbsoluteDiscounting(char_freq)
        char_prob=ConditionalProbDist(smooth_char_freq)
        write_csv(char_prob,'charprob.txt')
        
        tags_corpus=Condition([t for w,t in self.corpus])
        tag_freq=ConditionalFreqDist(self.tag_set,self.tag_set,tags_corpus)
        smooth_tag_freq=AbsoluteDiscounting(tag_freq)
        tag_prob=ConditionalProbDist(smooth_tag_freq)
        write_csv(tag_prob,'tagprob.txt')
        
        Pi=np.array([-0.26268660809250016,-3.14e+100,-3.14e+100,-1.4652633398537678])       
        write_csv(Pi,'startprob.txt')
        
    def token(self,sequence):
        return list(sequence)
    
    def cut(self,sentence):
        sentlist=self.token(sentence)
        obs=map_obs(self.char_set,sentlist)
        Pi=read_csv('startprob.txt')
        A=read_csv('tagprob.txt')
        B=read_csv('charprob.txt')
        prob,route=viterbi(obs,Pi,A,B)
        sequence=map_state(self.tag_set,route)
        
        seg=''
        for char,tag in zip(sentlist,sequence):
            if tag=='B':
                seg+=''.join(' '+char)
            elif tag=='M':
                seg+=''.join(char)
            elif tag=='E':
                seg+=''.join(char+' ')
            else:
                seg+=''.join(' '+char+' ')
        return seg.strip()

def get_set(symbols):
    return list(set(symbols))

def ConditionalFreqDist(labels_set,symbols_set,corpus):
    """
    type:list(tuple(str,str))
    rtype:numpy array
    """
    count=defaultdict(int)
    for (symbol,label) in corpus:
        count[(symbol,label)]+=1
        
    N=len(labels_set)
    M=len(symbols_set)
    arr=np.zeros((N,M),np.int)
        
    for i,symbol in enumerate(symbols_set):
        for j,label in enumerate(labels_set):
            arr[j][i]=count[(symbol,label)]
                          
    return arr

def Condition(labels):
    """
    type:list(label)
    rtype:bigram,list(tuple(str,str))
    """
    condition_label=[]
    i=0
    while i<len(labels)-1:
        condition_label.append((labels[i+1],labels[i]))
        i+=1
    return condition_label

def AbsoluteDiscounting(arr,discount=0.75):
    """
    绝对折扣平滑法
     对所有统计次数大于0的项减去-discount
    """
    if arr.size==len(arr):
        smooth_arr=np.array([0.0]*len(arr),np.float)
        N_zero=list(arr).count(0)
        for i in range(len(arr)):
            if arr[i]==0:
                smooth_arr[i]=(len(arr)-N_zero)*discount/N_zero
            else:
                smooth_arr[i]=arr[i]-discount
    else:
        (row,column)=arr.shape
        smooth_arr=np.zeros((row,column),np.float)
        for i in range(row):
            N_zero=list(arr[i]).count(0)
            for j in range(column):
                if arr[i][j]==0:
                    smooth_arr[i][j]=(column-N_zero)*discount/N_zero
                else:
                    smooth_arr[i][j]=arr[i][j]-discount
    return smooth_arr

def logprob(prob):
    return math.log(prob,2)

def ConditionalProbDist(arr):
    """
    type:numpy array,frequency distribution
    rtype:numpy array,probability distribution
    """
    (row,column)=arr.shape
    prob_arr=np.zeros((row,column),np.float)
    for i in range(row):
        N=sum(arr[i])
        for j in range(column):
            prob_arr[i][j]=logprob(arr[i][j]/N)
    return prob_arr

def write_csv(arr,filename):
    return np.savetxt("E:/corpus/"+filename,arr,delimiter=' ')

def read_csv(filename):
    return np.loadtxt(open("E:/corpus/"+filename,'r',encoding='utf-8'),delimiter=' ',skiprows=0)

def map_obs(symbols_set,obs):
    """
    建立字典索引
    rtype:list
    """
    symbol_index={}
    for i,symbol in enumerate(symbols_set):
        symbol_index[symbol]=i
    
    renew_obs=[]
    for s in obs:
        if s in symbol_index:
            renew_obs.append(s)
        else:
            renew_obs.append('UNK')
    return [symbol_index[symbol] for symbol in renew_obs]

def map_state(labels_set,route):
    label_index={}
    for i,label in enumerate(labels_set):
        label_index[i]=label
    return [label_index[i] for i in route]

def viterbi(obs,Pi,A,B):
    """
    维特比算法
    """
    N=len(A)
    T=len(obs)
    V=np.zeros((T,N),np.float)
    path=np.zeros((T,N),np.int)
    
    #初始化
    for i in range(N):
        V[0][i]=Pi[i]+B[i][obs[0]]
        path[0][i]=0
        
    #递推
    for t in range(1,T):
        for i in range(N):
            (max_prob,state)=max([(V[t-1][j]+A[j][i]+B[i][obs[t]],j) 
                                  for j in range(N)])
            V[t][i]=max_prob
            path[t][i]=state
          
    #最优路径的概率  
    last=[(V[T-1][i],i) for i in range(N)]
    prob,state=max(last)
    
    #逆向求最优路径
    route=[None]*T
    i = T - 1
    while i >= 0:
        route[i] = state
        state = path[i][state]
        i -= 1
    return (prob, route)

def tool():
    """
    人民日报语料库预处理
    rtype:list(list(tuple(word,tag)))
    >>>[[('迈向', 'v'),('充满', 'v'),('希望', 'n'),('的', 'u'),('新', 'a'),('世纪', 'n'),
        ('——', 'w'),('一九九八年', 't'),('新年', 't'),('讲话', 'n'),('（', 'w'),
        ('附', 'v'),('图片', 'n'),('１', 'm'),('张', 'q'),('）', 'w')],
       [('中共中央', 'nt'),('总书记', 'n'),('、', 'w'),('国家', 'n'),('主席', 'n'),
        ('江', 'nr'),('泽民', 'nr')]]
    """
    f=open("E:/corpus/ThePeople'sDaily199801.txt",'r',encoding='utf-8')
    
    tagged_sents=[]
    for line in f:
        raw_word_tag=[tuple(word.split('/')) for word in line.split()[1:]]
        word_tag=[(word.strip(),tag.strip()) for word,tag in raw_word_tag]
        tagged_sents.append(word_tag)    
    return tagged_sents

def word_seg(tagged_sents):
    """
    rtype:list(tuple(str,str)),list((word,tag))
    """
    tagged_words=[]
    for sent in tagged_sents:
        for word in sent:
            tagged_words.append(word)
    return [word for word,tag in tagged_words]

def del_english(seg_words):
    chinese_words=[]
    for word in seg_words:
        if re.match('^[\u4E00-\uFFFD]+$',word):
            chinese_words.append(word)
    return chinese_words

def map_tag(chinese_words):
    word_tag=[]
    for word in chinese_words:
        if len(word)==1:
            word_tag.append((word,'S'))
        else:
            tag=['B']+['M']*(len(word)-2)+['E']
            word_tag+=list(zip(list(word),tag))
    return word_tag

if __name__=='__main__':
    seg_words=word_seg(tool())
    char=del_english(seg_words)
    corpus=map_tag(char)
    segmenter=HMMSegmenter(corpus)
