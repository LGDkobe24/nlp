import re
import numpy as np
from collections import defaultdict
import math

class HMMTagger(object):
    def __init__(self,corpus):
        self.corpus=corpus
        self.tag_set=get_set([t for w,t in self.corpus])
        self.word_set=get_set([w for w,t in self.corpus])
          
    def train(self):
        word_freq=ConditionalFreqDist(self.tag_set,self.word_set,self.corpus)
        smooth_word_freq=AbsoluteDiscounting(word_freq)
        word_prob=ConditionalProbDist(smooth_word_freq)
        write_csv(word_prob,'emisprob.txt')
        
        tags_corpus=Condition([t for w,t in self.corpus])
        tag_freq=ConditionalFreqDist(self.tag_set,self.tag_set,tags_corpus)
        smooth_tag_freq=AbsoluteDiscounting(tag_freq)
        tag_prob=ConditionalProbDist(smooth_tag_freq)
        write_csv(tag_prob,'transprob.txt')
        
        tagged_sents=tool()
        start_tag=[tag for word,tag in [sent[0] for sent in tagged_sents if sent]]
        start_freq=FreqDist(self.tag_set,start_tag)
        smooth_start_freq=AbsoluteDiscounting(start_freq)
        start_prob=ProbDist(smooth_start_freq)
        write_csv(start_prob,'initprob.txt')       
    
    def tag(self,sentlist):
        obs=map_obs(self.word_set,sentlist)
        Pi=read_csv('initprob.txt')
        A=read_csv('transprob.txt')
        B=read_csv('emisprob.txt')
        prob,route=viterbi(obs,Pi,A,B)
        sequence=map_state(self.tag_set,route)
        result=''
        for word,tag in zip(sentlist,sequence):
            result+=''.join(' '+word+'/'+tag+' ')
        return result.strip()
        

def get_set(symbols):
    return list(set(symbols))

def FreqDist(symbols_set,symbols):
    """
    type:list(symbol)
    rtype:1-dim array
    """
    count=defaultdict(int)
    for symbol in symbols:
        count[symbol]+=1
    
    freq_arr=np.array([0.0]*len(symbols_set),np.int)
    for i,symbol in enumerate(symbols_set):
        freq_arr[i]=count[symbol]
    return freq_arr

def ConditionalFreqDist(labels_set,symbols_set,corpus):
    """
    type:list(tuple(str,str))
    rtype:numpy array
    """
    count=defaultdict(int)
    for (symbol,label) in corpus:
        count[(symbol,label)]+=1
        
    N=len(labels_set)
    M=len(symbols_set)
    arr=np.zeros((N,M),np.int)
        
    for i,symbol in enumerate(symbols_set):
        for j,label in enumerate(labels_set):
            arr[j][i]=count[(symbol,label)]
                          
    return arr

def Condition(labels):
    """
    type:list(label)
    rtype:bigram,list(tuple(str,str))
    """
    condition_label=[]
    i=0
    while i<len(labels)-1:
        condition_label.append((labels[i+1],labels[i]))
        i+=1
    return condition_label

def AbsoluteDiscounting(arr,discount=0.75):
    """
    绝对折扣平滑法
     对所有统计次数大于0的项减去-discount
    """
    if arr.size==len(arr):
        smooth_arr=np.array([0.0]*len(arr),np.float)
        N_zero=list(arr).count(0)
        for i in range(len(arr)):
            if arr[i]==0:
                smooth_arr[i]=(len(arr)-N_zero)*discount/N_zero
            else:
                smooth_arr[i]=arr[i]-discount
    else:
        (row,column)=arr.shape
        smooth_arr=np.zeros((row,column),np.float)
        for i in range(row):
            N_zero=list(arr[i]).count(0)
            for j in range(column):
                if arr[i][j]==0:
                    smooth_arr[i][j]=(column-N_zero)*discount/N_zero
                else:
                    smooth_arr[i][j]=arr[i][j]-discount
    return smooth_arr

def logprob(prob):
    return math.log(prob,2)

def ProbDist(freq_arr):
    """
    type:1-dim array,frequency distribution
    rtype:1-dim array,probability distribution
    """
    prob_arr=np.array([0.0]*len(freq_arr),np.float)
    N=sum(freq_arr)
    for i in range(len(freq_arr)):
        prob_arr[i]=logprob(freq_arr[i]/N)
    return prob_arr

def ConditionalProbDist(arr):
    """
    type:numpy array,frequency distribution
    rtype:numpy array,probability distribution
    """
    (row,column)=arr.shape
    prob_arr=np.zeros((row,column),np.float)
    for i in range(row):
        N=sum(arr[i])
        for j in range(column):
            prob_arr[i][j]=logprob(arr[i][j]/N)
    return prob_arr

def write_csv(arr,filename):
    return np.savetxt("E:/corpus/"+filename,arr,delimiter=' ')

def read_csv(filename):
    return np.loadtxt(open("E:/corpus/"+filename,'r',encoding='utf-8'),delimiter=' ',skiprows=0)

def map_obs(symbols_set,obs):
    """
    建立字典索引
    rtype:list
    """
    symbol_index={}
    for i,symbol in enumerate(symbols_set):
        symbol_index[symbol]=i
    
    renew_obs=[]
    for s in obs:
        if s in symbol_index:
            renew_obs.append(s)
        else:
            renew_obs.append('UNK')
    return [symbol_index[symbol] for symbol in renew_obs]

def map_state(labels_set,route):
    label_index={}
    for i,label in enumerate(labels_set):
        label_index[i]=label
    return [label_index[i] for i in route]

def viterbi(obs,Pi,A,B):
    """
    维特比算法
    """
    N=len(A)
    T=len(obs)
    V=np.zeros((T,N),np.float)
    path=np.zeros((T,N),np.int)
    
    #初始化
    for i in range(N):
        V[0][i]=Pi[i]+B[i][obs[0]]
        path[0][i]=0
        
    #递推
    for t in range(1,T):
        for i in range(N):
            (max_prob,state)=max([(V[t-1][j]+A[j][i]+B[i][obs[t]],j) 
                                  for j in range(N)])
            V[t][i]=max_prob
            path[t][i]=state
          
    #最优路径的概率  
    last=[(V[T-1][i],i) for i in range(N)]
    prob,state=max(last)
    
    #逆向求最优路径
    route=[None]*T
    i = T - 1
    while i >= 0:
        route[i] = state
        state = path[i][state]
        i -= 1
    return (prob, route)

def tool():
    """
    人民日报语料库预处理
    rtype:list(list(tuple(word,tag)))
    >>>[[('迈向', 'v'),('充满', 'v'),('希望', 'n'),('的', 'u'),('新', 'a'),('世纪', 'n'),
        ('——', 'w'),('一九九八年', 't'),('新年', 't'),('讲话', 'n'),('（', 'w'),
        ('附', 'v'),('图片', 'n'),('１', 'm'),('张', 'q'),('）', 'w')],
       [('中共中央', 'nt'),('总书记', 'n'),('、', 'w'),('国家', 'n'),('主席', 'n'),
        ('江', 'nr'),('泽民', 'nr')]]
    """
    f=open("E:/corpus/ThePeople'sDaily199801.txt",'r',encoding='utf-8')
    
    tagged_sents=[]
    for line in f:
        raw_word_tag=[tuple(word.split('/')) for word in line.split()[1:]]
        word_tag=[(word.strip(),tag.strip()) for word,tag in raw_word_tag]
        tagged_sents.append(word_tag)    
    return tagged_sents

def word_tag(tagged_sents):
    """
    rtype:list(tuple(str,str)),list((word,tag))
    """
    tagged_words=[]
    for sent in tagged_sents:
        for word in sent:
            tagged_words.append(word)
    return tagged_words

if __name__=='__main__':
    corpus=word_tag(tool())
    tagger=HMMTagger(corpus)
